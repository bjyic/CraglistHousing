{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "try:\n",
    "    from Queue import Queue  # PY2\n",
    "except ImportError:\n",
    "    from queue import Queue  # PY3\n",
    "from threading import Thread\n",
    "try:\n",
    "    from urlparse import urljoin  # PY2\n",
    "except ImportError:\n",
    "    from urllib.parse import urljoin  # PY3\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from requests.exceptions import RequestException\n",
    "from six import iteritems\n",
    "from six.moves import range\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import geopy.distance\n",
    "from geopy.distance import great_circle\n",
    "sites_url = 'http://www.craigslist.org/about/sites'\n",
    "\n",
    "\n",
    "def get_all_sites():\n",
    "    response = requests.get(sites_url)\n",
    "    response.raise_for_status()  # Something failed?\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    sites = set()\n",
    "\n",
    "    for box in soup.findAll('div', {'class': 'box'}):\n",
    "        for a in box.findAll('a'):\n",
    "            # Remove protocol and get subdomain\n",
    "            site = a.attrs['href'].rsplit('//', 1)[1].split('.')[0]\n",
    "            sites.add(site)\n",
    "\n",
    "    return sites\n",
    "ALL_SITES = get_all_sites()  # All the Craiglist sites\n",
    "RESULTS_PER_REQUEST = 100  # Craigslist returns 100 results per request\n",
    "\n",
    "def requests_get(*args, **kwargs):\n",
    "    import random\n",
    "    import time\n",
    "    \"\"\"\n",
    "    Retries if a RequestException is raised (could be a connection error or\n",
    "    a timeout).\n",
    "    \"\"\"\n",
    "\n",
    "    logger = kwargs.pop('logger', None)\n",
    "    try:\n",
    "        sleep_time= random.uniform(0.05,0.2)\n",
    "        time.sleep(sleep_time)\n",
    "        return requests.get(*args, **kwargs)\n",
    "    except RequestException as exc:\n",
    "        if logger:\n",
    "            logger.warning('Request failed (%s). Retrying ...', exc)\n",
    "        return requests.get(*args, **kwargs)\n",
    "    \n",
    "def get_list_filters(url):\n",
    "    list_filters = {}\n",
    "    response = requests_get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    for list_filter in soup.find_all('div', class_='search-attribute'):\n",
    "        filter_key = list_filter.attrs['data-attr']\n",
    "        filter_labels = list_filter.find_all('label')\n",
    "        options = [opt.text.strip() for opt in filter_labels]\n",
    "        list_filters[filter_key] = {'url_key': filter_key, 'value': options}\n",
    "    return list_filters\n",
    "\n",
    "\n",
    "from requests_futures.sessions import FuturesSession\n",
    "import re\n",
    "from retrying import retry\n",
    "import random\n",
    "import time\n",
    "class CraigslistBase(object):\n",
    "    \"\"\" Base class for all Craiglist wrappers. \"\"\"\n",
    "\n",
    "    url_templates = {\n",
    "        'base': 'http://%(site)s.craigslist.org',\n",
    "        'no_area': 'http://%(site)s.craigslist.org/search/%(category)s',\n",
    "        'area': 'http://%(site)s.craigslist.org/search/%(area)s/%(category)s'\n",
    "    }\n",
    "\n",
    "    default_site = 'sfbay'\n",
    "    default_category = None\n",
    "\n",
    "    base_filters = {\n",
    "        'query': {'url_key': 'query', 'value': None},\n",
    "        'search_titles': {'url_key': 'srchType', 'value': 'T'},\n",
    "        'has_image': {'url_key': 'hasPic', 'value': 1},\n",
    "        'posted_today': {'url_key': 'postedToday', 'value': 1},\n",
    "        'search_distance': {'url_key': 'search_distance', 'value': None},\n",
    "        'zip_code': {'url_key': 'postal', 'value': None},\n",
    "    }\n",
    "    extra_filters = {}\n",
    "\n",
    "    # Set to True to subclass defines the customize_results() method\n",
    "    custom_result_fields = False\n",
    "\n",
    "    sort_by_options = {\n",
    "        'newest': 'date',\n",
    "        'price_asc': 'priceasc',\n",
    "        'price_desc': 'pricedsc',\n",
    "    }\n",
    "\n",
    "    def __init__(self, site=None, area=None, category=None, filters=None,\n",
    "                 log_level=logging.WARNING):\n",
    "        # Logging\n",
    "        self.set_logger(log_level, init=True)\n",
    "\n",
    "        self.site = site or self.default_site\n",
    "        if self.site not in ALL_SITES:\n",
    "            msg = \"'%s' is not a valid site\" % self.site\n",
    "            self.logger.error(msg)\n",
    "            raise ValueError(msg)\n",
    "\n",
    "        if area:\n",
    "            if not self.is_valid_area(area):\n",
    "                msg = \"'%s' is not a valid area for site '%s'\" % (area, site)\n",
    "                self.logger.error(msg)\n",
    "                raise ValueError(msg)\n",
    "        self.area = area\n",
    "\n",
    "        self.category = category or self.default_category\n",
    "\n",
    "        url_template = self.url_templates['area' if area else 'no_area']\n",
    "        self.url = url_template % {'site': self.site, 'area': self.area,\n",
    "                                   'category': self.category}\n",
    "\n",
    "        list_filters = get_list_filters(self.url)\n",
    "\n",
    "        self.filters = {}\n",
    "        for key, value in iteritems((filters or {})):\n",
    "            try:\n",
    "                filter = (self.base_filters.get(key) or\n",
    "                          self.extra_filters.get(key) or\n",
    "                          list_filters[key])\n",
    "                if filter['value'] is None:\n",
    "                    self.filters[filter['url_key']] = value\n",
    "                elif isinstance(filter['value'], list):\n",
    "                    valid_options = filter['value']\n",
    "                    if not hasattr(value, '__iter__'):\n",
    "                        value = [value]  # Force to list\n",
    "                    options = []\n",
    "                    for opt in value:\n",
    "                        try:\n",
    "                            options.append(valid_options.index(opt) + 1)\n",
    "                        except ValueError:\n",
    "                            self.logger.warning(\n",
    "                                \"'%s' is not a valid option for %s\"\n",
    "                                % (opt, key)\n",
    "                            )\n",
    "                    self.filters[filter['url_key']] = options\n",
    "                elif value:  # Don't add filter if ...=False\n",
    "                    self.filters[filter['url_key']] = filter['value']\n",
    "            except KeyError:\n",
    "                self.logger.warning(\"'%s' is not a valid filter\", key)\n",
    "\n",
    "    def set_logger(self, log_level, init=False):\n",
    "        if init:\n",
    "            self.logger = logging.getLogger('python-craiglist')\n",
    "            self.handler = logging.StreamHandler()\n",
    "            self.logger.addHandler(self.handler)\n",
    "        self.logger.setLevel(log_level)\n",
    "        self.handler.setLevel(log_level)\n",
    "\n",
    "    def is_valid_area(self, area):\n",
    "        base_url = self.url_templates['base']\n",
    "        response = requests_get(base_url % {'site': self.site},\n",
    "                                logger=self.logger)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        sublinks = soup.find('ul', {'class': 'sublinks'})\n",
    "        return sublinks and sublinks.find('a', text=area) is not None\n",
    "\n",
    "    @retry(stop_max_attempt_number=100)\n",
    "    def get_results(self, limit=None, start=0, sort_by=None, geotagged=False):\n",
    "        \"\"\"\n",
    "        Get results from Craigslist based on the specified filters.\n",
    "        If geotagged=True, the results will include the (lat, lng) in the\n",
    "        'geotag' attrib (this will make the process a little bit longer).\n",
    "        \"\"\"\n",
    "\n",
    "        if sort_by:\n",
    "            try:\n",
    "                self.filters['sort'] = self.sort_by_options[sort_by]\n",
    "            except KeyError:\n",
    "                msg = (\"'%s' is not a valid sort_by option, \"\n",
    "                       \"use: 'newest', 'price_asc' or 'price_desc'\" % sort_by)\n",
    "                self.logger.error(msg)\n",
    "                raise ValueError(msg)\n",
    "\n",
    "        total_so_far = start\n",
    "        results_yielded = 0\n",
    "        total = 0\n",
    "\n",
    "        while True:\n",
    "            self.filters['s'] = start\n",
    "            response = requests_get(self.url, params=self.filters,\n",
    "                                    logger=self.logger)\n",
    "            self.logger.info('GET %s', response.url)\n",
    "            self.logger.info('Response code: %s', response.status_code)\n",
    "            response.raise_for_status()  # Something failed?\n",
    "\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            if not total:\n",
    "                totalcount = soup.find('span', {'class': 'totalcount'})\n",
    "                total = int(totalcount.text) if totalcount else 0\n",
    "\n",
    "            for row in soup.find_all('p', {'class': 'result-info'}):\n",
    "                if limit is not None and results_yielded >= limit:\n",
    "                    break\n",
    "                self.logger.debug('Processing %s of %s results ...',\n",
    "                                  total_so_far + 1, total)\n",
    "\n",
    "                link = row.find('a', {'class': 'hdrlnk'})\n",
    "                id = link.attrs['data-id']\n",
    "                name = link.text\n",
    "                url = urljoin(self.url, link.attrs['href'])\n",
    "\n",
    "                time = row.find('time')\n",
    "                if time:\n",
    "                    datetime = time.attrs['datetime']\n",
    "                else:\n",
    "                    pl = roprintw.find('span', {'class': 'pl'})\n",
    "                    datetime = pl.text.split(':')[0].strip() if pl else None\n",
    "                price = row.find('span', {'class': 'result-price'})\n",
    "                where = row.find('span', {'class': 'result-hood'})\n",
    "                if where:\n",
    "                    where = where.text.strip()[1:-1]  # remove ()\n",
    "                tags_span = row.find('span', {'class': 'result-tags'})\n",
    "                tags = tags_span.text if tags_span else ''\n",
    "\n",
    "                result = {#'id': id,\n",
    "                          'name': name,\n",
    "                          'url': url,\n",
    "                          'datetime': datetime,\n",
    "                          'price': price.text if price else None,\n",
    "                          'where': where,\n",
    "                          'has_image': 'pic' in tags#,\n",
    "                          # TODO: Look into this, looks like all show map now\n",
    "                          #'has_map': 'map' in tags,\n",
    "                #          'geotag': None\n",
    "                }\n",
    "                \n",
    "                #session=FuturesSession()\n",
    "                #future = session.get(url)\n",
    "                #response_detail= future.result()\n",
    "                #soup_response_detail=BeautifulSoup(response_detail.text,\"lxml\")\n",
    "                \n",
    "                future=requests_get(url)\n",
    "                soup_response_detail=BeautifulSoup(future.content,\"lxml\")\n",
    "                \n",
    "                try:\n",
    "                    mapaddress_pre=soup_response_detail.find_all(['div','p'], {'class': 'mapaddress'})\n",
    "                    mapaddress=''\n",
    "                    for i in mapaddress_pre:\n",
    "                        mapaddress=mapaddress+i.text.replace('\\n\\n','\\n')\n",
    "                    #mapaddress=mapaddress_pre[0].text.replace('\\n\\n','\\n')\n",
    "                    result.update({'mapaddress':mapaddress})\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                try:\n",
    "                    detail_content_pre=soup_response_detail.find_all(id='postingbody')\n",
    "                    detail_content=''\n",
    "                    for i in detail_content_pre:\n",
    "                        detail_content=detail_content+i.text.replace(unicode('\\n\\nQR Code Link to This Post\\n\\n\\n'),'')\\\n",
    "                        .replace('\\n\\n','\\n')\n",
    "\n",
    "                    #detail_content=detail_content_pre[0].text.replace(unicode('\\n\\nQR Code Link to This Post\\n\\n\\n'),'')\\\n",
    "                    #.replace('\\n\\n','\\n')\n",
    "                    result.update({'detail_content':detail_content})\n",
    "\n",
    "\n",
    "                    chinese_flag=re.findall(ur'[\\u4e00-\\u9fff]+', detail_content)\n",
    "\n",
    "\n",
    "                    if len(chinese_flag)>0:\n",
    "                        chinese_content=True\n",
    "                    else:\n",
    "                        chinese_content=False\n",
    "                    result.update({'chinese_content':chinese_content})\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                \n",
    "                \n",
    "                try:\n",
    "                    attrgroup_pre=soup_response_detail.find_all(['div','p'], {'class': 'attrgroup'})\n",
    "                    attr_text=''\n",
    "                    for tmp in attrgroup_pre:\n",
    "                        attr_text=attr_text+tmp.text.replace('\\n\\n','\\n')\n",
    "                    result.update({'attr_text':attr_text})\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                \n",
    "                geolocation=geolocation=soup_response_detail.find_all(['div','p'], {'class': 'viewposting'})\n",
    "                try:\n",
    "                    geolocation=geolocation[0]\n",
    "                    geolocation_latitude=geolocation.attrs['data-latitude']\n",
    "                    geolocation_longitude=geolocation.attrs['data-longitude']\n",
    "                    result.update({'geolocation_latitude':geolocation_latitude})\n",
    "                    result.update({'geolocation_longitude':geolocation_longitude})\n",
    "                    \n",
    "                    #result.update({'geo_location':[geolocation_latitude,geolocation_longitude]})\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                \n",
    "                if self.custom_result_fields:\n",
    "                    self.customize_result(result, row)\n",
    "\n",
    "                if geotagged and result['has_map']:\n",
    "                    self.geotag_result(result)\n",
    "\n",
    "                yield result\n",
    "                results_yielded += 1\n",
    "                total_so_far += 1\n",
    "\n",
    "            if results_yielded == limit:\n",
    "                break\n",
    "            if (total_so_far - start) < RESULTS_PER_REQUEST:\n",
    "                break\n",
    "            start = total_so_far\n",
    "\n",
    "    def customize_result(self, result, html_row):\n",
    "        \"\"\" Add custom/delete/alter fields to result. \"\"\"\n",
    "        pass  # Override in subclass to add category-specific fields.\n",
    "\n",
    "    def geotag_result(self, result):\n",
    "        \"\"\" Adds (lat, lng) to result. \"\"\"\n",
    "\n",
    "        self.logger.debug('Geotagging result ...')\n",
    "\n",
    "        if result['has_map']:\n",
    "            response = requests_get(result['url'], logger=self.logger)\n",
    "            self.logger.info('GET %s', response.url)\n",
    "            self.logger.info('Response code: %s', response.status_code)\n",
    "\n",
    "            if response.ok:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                map = soup.find('div', {'id': 'map'})\n",
    "                if map:\n",
    "                    result['geotag'] = (float(map.attrs['data-latitude']),\n",
    "                                        float(map.attrs['data-longitude']))\n",
    "\n",
    "        return result\n",
    "\n",
    "    def geotag_results(self, results, workers=8):\n",
    "        \"\"\"\n",
    "        Add (lat, lng) to each result. This process is done using N threads,\n",
    "        where N is the amount of workers defined (default: 8).\n",
    "        \"\"\"\n",
    "\n",
    "        results = list(results)\n",
    "        queue = Queue()\n",
    "\n",
    "        for result in results:\n",
    "            queue.put(result)\n",
    "\n",
    "        def geotagger():\n",
    "            while not queue.empty():\n",
    "                self.logger.debug('%s results left to geotag ...',\n",
    "                                  queue.qsize())\n",
    "                self.geotag_result(queue.get())\n",
    "                queue.task_done()\n",
    "\n",
    "        threads = []\n",
    "        for _ in range(workers):\n",
    "            thread = Thread(target=geotagger)\n",
    "            thread.start()\n",
    "            threads.append(thread)\n",
    "\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "        return results\n",
    "\n",
    "    @classmethod\n",
    "    def show_filters(cls, category=None):\n",
    "        print('Base filters:')\n",
    "        for key, options in iteritems(cls.base_filters):\n",
    "            value_as_str = '...' if options['value'] is None else 'True/False'\n",
    "            print('* %s = %s' % (key, value_as_str))\n",
    "        print('Section specific filters:')\n",
    "        for key, options in iteritems(cls.extra_filters):\n",
    "            value_as_str = '...' if options['value'] is None else 'True/False'\n",
    "            print('* %s = %s' % (key, value_as_str))\n",
    "        url = cls.url_templates['no_area'] % {\n",
    "            'site': cls.default_site,\n",
    "            'category': category or cls.default_category,\n",
    "        }\n",
    "        list_filters = get_list_filters(url)\n",
    "        for key, options in iteritems(list_filters):\n",
    "            value_as_str = ', '.join([repr(opt) for opt in options['value']])\n",
    "            print('* %s = %s' % (key, value_as_str))\n",
    "\n",
    "            \n",
    "class CraigslistHousing(CraigslistBase):\n",
    "    \"\"\" Craigslist housing wrapper. \"\"\"\n",
    "\n",
    "    default_category = 'hhh'\n",
    "    custom_result_fields = True\n",
    "\n",
    "    extra_filters = {\n",
    "        'private_room': {'url_key': 'private_room', 'value': 1},\n",
    "        'private_bath': {'url_key': 'private_bath', 'value': 1},\n",
    "        'cats_ok': {'url_key': 'pets_cat', 'value': 1},\n",
    "        'dogs_ok': {'url_key': 'pets_dog', 'value': 1},\n",
    "        'min_price': {'url_key': 'min_price', 'value': None},\n",
    "        'max_price': {'url_key': 'max_price', 'value': None},\n",
    "        'min_ft2': {'url_key': 'minSqft', 'value': None},\n",
    "        'max_ft2': {'url_key': 'maxSqft', 'value': None},\n",
    "        'min_bedrooms': {'url_key': 'min_bedrooms', 'value': None},\n",
    "        'max_bedrooms': {'url_key': 'max_bedrooms', 'value': None},\n",
    "        'min_bathrooms': {'url_key': 'min_bathrooms', 'value': None},\n",
    "        'max_bathrooms': {'url_key': 'max_bathrooms', 'value': None},\n",
    "        'no_smoking': {'url_key': 'no_smoking', 'value': 1},\n",
    "        'is_furnished': {'url_key': 'is_furnished', 'value': 1},\n",
    "        'wheelchair_acccess': {'url_key': 'wheelchaccess', 'value': 1},\n",
    "    }\n",
    "\n",
    "    def customize_result(self, result, html_row):\n",
    "        housing_info = html_row.find('span', {'class': 'housing'})\n",
    "        # Default values\n",
    "        result.update({'bedrooms': None, 'area': None})\n",
    "        if housing_info:\n",
    "            for elem in housing_info.text.split('-'):\n",
    "                elem = elem.strip()\n",
    "                if elem.endswith('br'):\n",
    "                    # Don't convert to int, too risky\n",
    "                    result['bedrooms'] = elem[:-2]\n",
    "                if elem.endswith('2'):\n",
    "                    result['area'] = elem\n",
    "                    \n",
    "def attr_space(x):\n",
    "    try:\n",
    "        a=re.search('\\d*ft2',x)\n",
    "        b=a.group(0)\n",
    "        return b\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "def attr_type(x):\n",
    "    try:\n",
    "        a=re.search('\\d\\w\\w\\s*/\\s*\\d\\w\\w',x)\n",
    "        b=a.group(0)\n",
    "        return b\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "apa_east_bay = CraigslistHousing(site='sfbay', area='eby', category='apa',)\n",
    "apa_north_bay = CraigslistHousing(site='sfbay', area='nby', category='apa',)\n",
    "apa_penninsula = CraigslistHousing(site='sfbay', area='pen', category='apa',)\n",
    "apa_san_francisco = CraigslistHousing(site='sfbay', area='sfc', category='apa',)\n",
    "apa_santa_cruz = CraigslistHousing(site='sfbay', area='scz', category='apa',)\n",
    "apa_south_bay = CraigslistHousing(site='sfbay', area='sby', category='apa',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "now = datetime.datetime.now()\n",
    "output_date=str(now.year)+str(now.month)+str(now.day-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_apa_east_bay = pd.DataFrame(apa_east_bay.get_results(sort_by='newest', limit=2000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_apa_north_bay=pd.DataFrame(apa_north_bay.get_results(sort_by='newest', limit=2000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_apa_penninsula=pd.DataFrame(apa_penninsula.get_results(sort_by='newest', limit=2000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_apa_san_francisco=pd.DataFrame(apa_san_francisco.get_results(sort_by='newest', limit=2000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_apa_santa_cruz=pd.DataFrame(apa_santa_cruz.get_results(sort_by='newest', limit=2000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_apa_south_bay=pd.DataFrame(apa_south_bay.get_results(sort_by='newest', limit=2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_full=pd.concat([df_apa_east_bay,df_apa_north_bay,df_apa_penninsula,df_apa_san_francisco,df_apa_santa_cruz,df_apa_south_bay]).drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_full['attr_type']=data_full['attr_text'].apply(lambda x: attr_type(x))\n",
    "data_full['attr_space']=data_full['attr_text'].apply(lambda x: attr_space(x))\n",
    "data_full['lat_long'] = data_full[['geolocation_latitude', 'geolocation_longitude']].apply(tuple, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.remove(r\"C:\\Users\\cnyi\\Box Sync\\Github\\Python-code\\CraglistHousing\\update_apa_data_\"+output_date+\".csv\")\n",
    "except OSError:\n",
    "              pass\n",
    "    \n",
    "data_full.to_csv(r\"C:\\Users\\cnyi\\Box Sync\\Github\\Python-code\\CraglistHousing\\update_apa_data_\"+output_date+\".csv\",encoding ='utf8', index=False)#,index_label ='index')#, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def great_circle_crime_distance(x,y):\n",
    "    import sys\n",
    "    try:\n",
    "        distance=great_circle(x,y).miles\n",
    "    except:\n",
    "        distance=9999\n",
    "        #print sys.exc_info()\n",
    "        #break\n",
    "    return distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crime_data=pd.read_csv(r'C:\\Users\\cnyi\\Box Sync\\Github\\Python-code\\CraglistHousing\\Police_Department_Incidents_Current_Year_2017.csv')\n",
    "cnt_list=[]\n",
    "for i in data_full['lat_long']:\n",
    "    cnt=0\n",
    "    a=crime_data['Location'].apply(lambda x: great_circle_crime_distance(i,x)<3)\n",
    "    l=len(crime_data['Location'][a])\n",
    "    cnt_list.append(l) \n",
    "\n",
    "    se = pd.Series(cnt_list)\n",
    "data_full['crime_count'] = se.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.remove(r\"C:\\Users\\cnyi\\Box Sync\\Github\\Python-code\\CraglistHousing\\update_apa_data_crime\"+output_date+\".csv\")\n",
    "except OSError:\n",
    "              pass\n",
    "    \n",
    "data_full.to_csv(r\"C:\\Users\\cnyi\\Box Sync\\Github\\Python-code\\CraglistHousing\\update_apa_data_crime\"+output_date+\".csv\",encoding ='utf8', index=False)#,index_label ='index')#, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
